input {
  udp {
    port => 1514
    type => Barracuda
  }
}
filter {
	if [type] == "Barracuda" {
	ruby {
		init => "
		HEADER_FIELDS = ['cef_version','Vendor','Product','DeviceVersion','SignatureId','EventName','Severity']
	      	#event_new = LogStash::Event.new
		      def store_header_field(event,field_name,field_data)
                          #Special Condition for CONNECTION LOGS in ADC as in syslog EventName, Severity headers are missing
                          if field_data =~ /cat\=CONN/ 
                             if field_name == 'EventName'
                                 field_data = ''
                             end
                             if field_name == 'Severity'
                                 field_data = ''
                             end
                          end
                          #Unescape pipes and backslash in header fields
		          event.set(field_name,field_data.gsub(/\\\|/, '|').gsub(/\\\\/, '\\')) unless field_data.nil?
		      end
		
      		"
		code => "

	        if event.get('[message][0]') == '\"'
		    event.set('[message]' , event.get('[message]')[1..-2])
		end
	        
          split_data = event.get('[message]').to_s.split /(?<=[^\\]\\\\)[\|]|(?<!\\)[\|]/
      		HEADER_FIELDS.each_with_index do |field_name, index|
      		   store_header_field(event,field_name,split_data[index])
      		end
          msg = 
                if ( not split_data.nil? or split_data.empty? ) and split_data.length > 1
                   split_data.reject(&:empty?)
                   #split_data[HEADER_FIELDS.size..-1].join('|')
                   split_data.join('|')
                end
      		if split_data.length > 1
                   if event.get('cef_version').include? ' '
      		       split_cef_version= event.get('cef_version').rpartition(' ')
       	               event.set('syslog', split_cef_version[0])
      	               event.set('cef_version',split_cef_version[2])
                   end
      	        end
                if event.get('cef_version') =~ /^CEF:/
      		    event.set('cef_version', event.get('cef_version').sub(/^CEF:/, ''))  unless event.get('cef_version').nil?
                end
     		if not msg.nil? and msg.include? '='
      		   msg = msg.strip
      		   # If the last KVP has no value, add an empty string, this prevents hash errors below
                   if msg.end_with?('=')
      		      msg=msg + ' ' unless msg.end_with?('\=')
	           end

	           # Now parse the key value pairs into it
                    msg = msg.split(/[ ]*([\w\.]+)=/)
                   #msg = msg.split(/ ([\w\.]+)=/)
                   msg.shift()
                   Hash[*msg].each{ |k, v| event.set(k,v.gsub(/\\=/, '=').gsub(/\\\\/, '\\')) unless v.nil? }
	           hash2 = event.to_hash
                hash2.each { |key2,value2|
                       logger.info('Key 2:', 'value' => key2)
                       logger.info('Value 2 :', 'value' => value2)
                }
                end
	        "
  remove_field => ['message']
	}
	# Filtering LogFields which are common to all Log Types
  mutate {
    convert => {"Severity" => "integer" } 
  }
 grok {
   match => {"cef_version" => ".+\:%{INT:cef_version:int}" }
   overwrite => ["cef_version"]
 }
  grok {
    match => {"rt" => "^\s*%{DATA:rt}\s*$" }
    overwrite => ["rt"]
  }
  mutate {
    gsub => ["StartTime", '\"', ""]
  }
  grok {
    match => {"StartTime" => "^\s*%{DATA:StartTime}\s*$" }
    overwrite => ["StartTime"]
  }
  date {
    match => ["StartTime","MMM dd YYYY HH:mm:ss"] 
    target => "StartTime"
  }
  date {
    match => ["DeviceReceiptTime","UNIX_MS"]
    target => "rt"
  }
# LogType Specific Filtering
if [cat] == "WF" {
    grok {
      match => {"cn2" => "%{INT:ProxyPort:int}" }
      overwrite => ["ProxyPort"]
    }
    
    grok {
      match => {"dpt" => "%{INT:ServicePort:int}" }
      overwrite => ["ServicePort"]
    }
    grok {
      match => {"spt" => "%{INT:ClientPort:int}" }
      overwrite => ["ClientPort"]
    }
    geoip {
      source => "src"
      target => "geoip"
      #database => "/etc/logstash/GeoLiteCity.dat"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
    }
 } else if [cat] == "TR" {
    grok {
      match => {"dpt" => "%{INT:ServicePort:int}" }
      overwrite => ["ServicePort"]
    }
    grok {
      match => {"cn2" => "%{INT:CacheHit:int}" }
      overwrite => ["CacheHit"]
    }
    grok {
      match => {"cn3" => "%{INT:ProxyPort:int}" }
      overwrite => ["ProxyPort"]
    }
    grok {
      match => {"flexNumber1" => "%{INT:ServerTime:int}" }
      overwrite => ["ServerTime"]
    }
    grok {
      match => {"flexNumber2" => "%{INT:TimeTaken:int}" }
      overwrite => ["TimeTaken"]
    }
    
    
    grok {
      match => {"dpt" => "%{INT:ServerPort:int}" }
      overwrite => ["ServerPort"]
    }
    grok {
      match => {"in" => "%{INT:BytesReceived:int}" }
      overwrite => ["BytesReceived"]
    }
    grok {
      match => {"out" => "%{INT:BytesSent:int}" }
      overwrite => ["BytesSent"]
    }
    grok {
      match => {"spt" => "%{INT:ClientPort:int}" }
      overwrite => ["ClientPort"]
    }
    geoip {
      source => "src"
      target => "geoip"
      #database => "/etc/logstash/GeoLiteCity.dat"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
    }
  }
if "_rubyexception" in [tags] {
     drop {}
  }
  if ![cat] {
     drop {}
  }

	}
}
output {
	file {
      path => "/home/labuser/output.txt"
    }
        microsoft-logstash-output-azure-loganalytics {
                workspace_id => "f669f133-811c-461b-be3f-49459d15bf8d"
                workspace_key => "d1LSaCkbCFG75Ucz0MM1OVVTlWHsGBCgxc63ajjMnU1vlB8kNsROJFWsjKa8+nYS40ah+egn3yyh/wKEhZ5B5Q=="
                custom_log_table_name => "WAAS_MS_Sentinel"
                plugin_flush_interval => 5
        }
}
